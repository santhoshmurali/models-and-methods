{
    "collab_server" : "",
    "contents" : "#Logistic Regression PKV Sir approach\n#Required Libraries \nlibrary(googleVis)\nlibrary(corrgram)\nlibrary(Deducer) #ROC curve\nlibrary(sqldf)\nlibrary(lmtest) #log likelihood\nlibrary(pscl) #Pseudo r squared\n\n#Read the Data from CSV File\ngetwd()\nsetwd(\"C:/Home/Work/Data Science/models-and-methods\")\nmydata = read.table(file = \"Logit/Data/CellPhone.csv\", header = T, sep = \",\")\nnames(mydata)[1] = \"Churn\"\nsapply(mydata, function(x) summary(x))\n\n#Check for Correlation \ncorData  = cor(mydata[,-1],method = \"pearson\")\ncorData\ncorrgram(mydata[,-1], order = TRUE, lower.panel = panel.pie, upper.panel = panel.pie, text.panel = panel.txt, main=\"Correlation Matirx\", diag.panel = panel.minmax)\n#Highly correlated variables\n# Data Plan X Data Usage\n# Data Plan X Monthly Charge\n# Monthly Charge X DayMins\n# Monthly Charge X OverageMins\n\n#Performing Linear Regression\nregression = lm(Churn~., data = mydata)\nsummary(regression)\nlrPredict = predict(regression)\ndf_lrPredict = as.data.frame(lrPredict)\nsqldf('select count(1) from df_lrPredict where lrPredict>1 or lrPredict <0 ')\n#There are 474 observations, whose probability is beyond the realistic range of 0 to 1. Hence, we are using Logistic regression, instead of Linear regression.\n\n#Predicting the Churn using Logistic Regression\n#Splitting the data into 70:30\n# 70% for Training\n# 30% for Testing\nset.seed(45)\nsplitPropotion = sample(nrow(mydata),nrow(mydata)*.7)\nmydata_train = mydata[splitPropotion,]\nmydata_test = mydata[-splitPropotion,]\n#As we know there are 4 variables that are highly correlated in the following order, we will use them as interaction effect while defining the equation.\n#Highly correlated variables\n# Data Plan X Data Usage\n# Data Plan X Monthly Charge\n# Monthly Charge X DayMins\n# Monthly Charge X OverageMins\n\n\n#Model Equation and Model\nlogitModel = glm(Churn~AccountWeeks+ContractRenewal+DataPlan+DataUsage+CustServCalls+DayMins+DayCalls+MonthlyCharge+OverageFee+RoamMins+DataPlan*DataUsage+DataPlan*MonthlyCharge+MonthlyCharge*DayMins+MonthlyCharge*OverageFee,data = mydata_train, family = binomial)\n\n#Testing for Goodness of fit\n### Log likelihood Ratio Test\nlrtest(logitModel)\n## The overall test of the model is significant based on the Chisq test and it is highly significant. This tells that likelihood of any customer churning out is heavily dependent on all the variables or at least one variable.\n\n###Pseudo R Square\npR2(logitModel)\n## McFadden RSquare is 30%, which means that 30% of uncertainty of intercept only model (model2 in likelihood ration test) has been explained by full model (model1).\n## goodness of fit is reasonable\n\n#Summary of the model\nsummary(logitModel)\n# Customers who has recently renewed the contract,  with significant usage of data during the day, customers’ whose monthly charges are more, and who has paid significantly large amount in past 12 months is negatively impacting the churn, which means they are churning out. Especially recent contact renewals.  \n# Customers having data services and also users of roaming services are positively impacting the churn. Especially customers with Data as they are Statistically Significant. \n# Customers who frequently call the call centers seems to be well informed and feel more loyal, which is what reflecting in Positive impact with high Significance. \n# all the correlated variables are statistically very significant and also most of them are positive.\n# Deviance has dropped sharply from Null to Residual, that signifies that Independent variables are impacting the churn strongly\n# Company has to look in the existing contact renewal model to control churn, and also revisit and improvise its data plan so that we can bring in more customers to data and control churn.\n# There is a strong correlation between Data Plan and Usage and Charges, also, significantly impacting the churn. Company might want to look into pricing structure as well.\n\n\n#Odds and Probability\nodds = exp(coef(logitModel))\nprob = odds/(odds + 1)\nodds_and_prob = cbind(as.data.frame(odds),as.data.frame(prob)[,1])\nnames(odds_and_prob)[2] = \"prob\"\nodds_and_prob\n# If data plan changes by 1 unit, the odds for churn is impacted by 8616 times, compared to the loyal customers.\n# Similar trend we can see for DataPlan with DataUsage\nconfint(logitModel)\n# Confident interval also tells the same story\n\n\n# Lets us now Predict the Test data using the model\nlogitPredcit = predict(logitModel,newdata = mydata_test, type = \"response\")\ndf_logitPredcit = as.data.frame(logitPredcit)\nsqldf('select * from df_logitPredcit where logitPredcit > 1 or logitPredcit < 0')\n# Since we have converted them to Probability based on Exponentials, we do not see negative probability. This way we have brought all the likelihood with the realistic range.\n\n# Setting the cut-off value.\nChurnPredicted = as.data.frame(ifelse(logitPredcit>.5,1,0))\nnames(ChurnPredicted)[1] = \"ChurnPredicted\"\n# Testing the Performance or Accuraracy of Fit\n## Confusion matrix or classification table\nglm_CM=table(mydata_test$Churn, ChurnPredicted$ChurnPredicted)\nglm_CM = as.matrix(glm_CM)\nglm_TP = glm_CM[1,1]\nglm_FN = glm_CM[1,2]\nglm_FP = glm_CM[2,1]\nglm_TN = glm_CM[2,2]\n\n# accuracy = (TP+TN)/(TP+FN+FP+TN)\nglm_accuracy  = (glm_TP + glm_TN)/(glm_TP+glm_TN+glm_FP+glm_FN)\nglm_accuracy\n# model is 88% Accurate\n\n# Specificity = TN/(TN+FP)\nglm_Specificity = glm_TN/(glm_TN+glm_FP)\nglm_Specificity\n# 35% of time model predicted churning customers correctly.\n\n# Sensitivity =TP/(TP+FN)\nglm_Sensitivity = glm_TP/(glm_TP+glm_FN)\nglm_Sensitivity\n# 98% of times model predicted non churning customers correctly.\n\n##ROC Curve: Receiver Operating Characteristic(ROC) \n# model’s performance by evaluating the trade offs between true positive rate (sensitivity) and false positive rate(1- specificity). \n# with assumption of cut of point at .5\n#.90-1 = excellent (A)\n#.80-.90 = good (B)\n#.70-.80 = fair (C)\n#.60-.70 = poor (D)\n#.50-.60 = fail (F)\nrocplot(logitModel)\n#Here we have AUC - 0.86, which is a good predictive model\n\n# Conclusion\n# using the equation \"AccountWeeks+ContractRenewal+DataPlan+DataUsage+CustServCalls+DayMins+DayCalls+MonthlyCharge+OverageFee+RoamMins+DataPlan*DataUsage+DataPlan*MonthlyCharge+MonthlyCharge*DayMins+MonthlyCharge*OverageFee\n# we should be able to predict the churning customer 86% of time accurately.\n# Organization must look into Data Plan , Usage and Charges to control the churn.\n\n\n\n",
    "created" : 1498965462858.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3078400719",
    "id" : "6BCD1701",
    "lastKnownWriteTime" : 1498979080,
    "last_content_update" : 1498979081009,
    "path" : "C:/Home/Work/Data Science/models-and-methods/Logit/LogitPKVSir.R",
    "project_path" : "Logit/LogitPKVSir.R",
    "properties" : {
        "notebook_format" : "word_document",
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}