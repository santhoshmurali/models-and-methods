UP.Analysis$Major<-UP.Analysis$INC+UP.Analysis$BJP+UP.Analysis$SP+UP.Analysis$BSP
UP.Analysis$Graduate<-ifelse(UP.Analysis$Education =="Graduate",1,0)
UP.Analysis$Doctorate<-ifelse(UP.Analysis$Education=="Doctorate",1,0)
UP.Analysis$Graduate.Professional<-ifelse(UP.Analysis$Education=="Graduate Professional",1,0)
UP.Analysis$Post.Graduate<-ifelse(UP.Analysis$Education=="Post Graduate",1,0)
UP.Analysis$Grad=UP.Analysis$Graduate+UP.Analysis$Doctorate+UP.Analysis$Graduate.Professional + UP.Analysis$Post.Graduate
UP.Analysis$GEN<-ifelse(UP.Analysis$AC_TYPE=="GEN",1,0)
UP.Analysis$Winner<-ifelse(UP.Analysis$POSITION=="1",1,0)
UP.Analysis$Serious<-ifelse(UP.Analysis$ser>0,1,0)
UP.Analysis$Female<-ifelse(UP.Analysis$Gender=="F",1,0)
UP.Analysis$Win.Party<-ifelse(UP.Analysis$Winner==1,UP.Analysis$Party,0)
UP.Analysis$Vote.Share<-(UP.Analysis$Votes)/UP.Analysis$Polled
UP.Analysis$Turnout<-UP.Analysis$Polled/UP.Analysis$Electors
UP.Analysis$Vote.Share<-(UP.Analysis$Votes)/UP.Analysis$Polled
UP.Analysis$Turnout<-UP.Analysis$Polled/UP.Analysis$Electors
levels(UP.Analysis$Education )
UP.Analysis$Qual<-as.character(UP.Analysis$Education)
UP.Analysis$Qual[UP.Analysis$Qual=="10th Pass"]<-"School.Complete"
UP.Analysis$Qual[UP.Analysis$Qual=="12th Pass"]<-"School.Complete"
UP.Analysis$Qual[UP.Analysis$Qual=="5th Pass"]<-"Secondary"
UP.Analysis$Qual[UP.Analysis$Qual=="8th Pass"]<-"Secondary"
UP.Analysis$Qual[UP.Analysis$Qual=="Doctorate"]<-"Graduate"
UP.Analysis$Qual[UP.Analysis$Qual=="Graduate"]<-"Graduate"
UP.Analysis$Qual[UP.Analysis$Qual=="Graduate Professional"]<-"Graduate"
UP.Analysis$Qual[UP.Analysis$Qual=="Iliterate"]<-"No.Schooling"
UP.Analysis$Qual[UP.Analysis$Qual=="Literate"]<-"No.Schooling"
UP.Analysis$Qual[UP.Analysis$Qual=="Others"]<-"School.Complete"
UP.Analysis$Qual[UP.Analysis$Qual=="Post Graduate"]<-"Graduate"
UP.Analysis$Qual<-as.factor((UP.Analysis$Qual))
UP.Analysis$years.studied<-ifelse(UP.Analysis$Education=="Illiterate",0,ifelse(UP.Analysis$Education=="Literate",1,
ifelse(UP.Analysis$Education=="5th Pass",5,
ifelse(UP.Analysis$Education=="8th Pass",8,ifelse(UP.Analysis$Education=="10th Pass",10,
ifelse(UP.Analysis$Education=="12th Pass",12,ifelse(UP.Analysis$Education=="Graduate",
15,ifelse(UP.Analysis$Education=="Graduate Professional",16,ifelse(UP.Analysis$Education=="Post Graduate",17,
ifelse(UP.Analysis$Education=="Doctorate",22,5))))))))))
sum(UP.Analysis$Winner)
UP.Analysis<-transform(UP.Analysis, no.winner = ave(UP.Analysis$Winner, UP.Analysis$AC_NAME,
FUN = max))
UP.Analysis<-subset(UP.Analysis,UP.Analysis$no.winner!=0)
sum(UP.Analysis$Winner)
UP.Analysis <- transform(UP.Analysis,
rank = ave(UP.Analysis$Total.Assets, UP.Analysis$AC_NAME,
FUN = function(x) rank(-x, ties.method = "first")))
plot(gvisTable(UP.Analysis))
UP.Analysis <- transform(UP.Analysis,
rank = ave(UP.Analysis$Total.Assets, UP.Analysis$AC_NAME,
FUN = function(x) rank(-x, ties.method = "first")))
UP.Analysis$Asset.Rank<-UP.Analysis$rank
normalize<-function(x){
+return((x-min(x))/(max(x)-min(x)))}
UP.Analysis<-transform(UP.Analysis, mean.assets=ave(UP.Analysis$Total.Assets, UP.Analysis$AC_NAME,
FUN = normalize))
UP.Analysis<-transform(UP.Analysis, mean.age=ave(UP.Analysis$Age, UP.Analysis$AC_NAME,
FUN = normalize))
UP.Analysis<-transform(UP.Analysis, mean.studied=ave(UP.Analysis$years.studied, UP.Analysis$AC_NAME,
FUN = normalize))
UP.Analysis$Rich<-ifelse(UP.Analysis$Asset.Rank==1,"R1",
ifelse(UP.Analysis$Asset.Rank==2,"R2",
ifelse(UP.Analysis$Asset.Rank==3,"R3","NR")))
UP.Analysis$Rich<-as.factor(UP.Analysis$Rich)
UP.Analysis$Vote.Share<-UP.Analysis$Votes/UP.Analysis$Polled
UP.Analysis$Status<-as.factor(ifelse(UP.Analysis$Winner==1,"Win","No"))
UP.Analysis$Position<-as.factor(ifelse(UP.Analysis$POSITION==1,"P1",ifelse(UP.Analysis$POSITION==2,"P2",
ifelse(UP.Analysis$POSITION==3,"P3","Rest"))))
qplot(mean.assets,  Asset.Rank, colour = Status, data=UP.Analysis)
qplot(mean.assets,  Asset.Rank, colour = Position, data=UP.Analysis)
qplot(mean.assets,  Asset.Rank, colour = Status, data=UP.Analysis)
qplot(mean.assets,  Asset.Rank, colour = Position, data=UP.Analysis)
qplot(mean.assets,  mean.age, colour = Status, data=UP.Analysis)
qplot(mean.assets,  mean.age, colour = Position, data=UP.Analysis)
qplot(mean.assets,  mean.age, colour = Rich, data=UP.Analysis)
qplot(mean.assets,  Vote.Share, colour = Rich, data=UP.Analysis)
qplot(mean.age,  Vote.Share, colour = Rich, data=UP.Analysis)
qplot(mean.assets,  mean.studied, colour = Status, data=UP.Analysis)
qplot(Asset.Rank,  mean.studied, colour = Position, data=UP.Analysis)
UP.Scatter<-subset(UP.Analysis[,c(37:51,54)])
pairs.panels(UP.Scatter[1:15], gap=0, bg=c("red", "blue","green",'orange')[UP.Scatter$Position], pch=21)
boxplot(UP.Analysis$Vote.Share  ~UP.Analysis$Rich)
aov.Rich<-aov(UP.Analysis$Vote.Share  ~UP.Analysis$Rich)
summary(aov.Rich)
tk.1<-TukeyHSD(aov.Rich)
tk.1
plot(tk.1)
boxplot(UP.Analysis$Vote.Share  ~UP.Analysis$Qual)
aov.Qualification<-aov(UP.Analysis$Vote.Share  ~UP.Analysis$Qual)
summary(aov.Qualification)
tk.3<-TukeyHSD(aov.Qualification)
tk.3
plot(tk.3)
UP.Analysis<-transform(UP.Analysis, dist.age=ave(UP.Analysis$Age, UP.Analysis$DIST_NAME
))
UP.Analysis<-transform(UP.Analysis, dist.TA=ave(UP.Analysis$Total.Assets, UP.Analysis$DIST_NAME))
UP.Analysis<-transform(UP.Analysis, dist.Grad=ave(UP.Analysis$Grad, UP.Analysis$DIST_NAME))
UP.Analysis<-transform(UP.Analysis, dist.Fem=ave(UP.Analysis$Female, UP.Analysis$DIST_NAME))
UP.Analysis<-transform(UP.Analysis, dist.Serious=ave(UP.Analysis$ser, UP.Analysis$DIST_NAME))
UP.Analysis<-transform(UP.Analysis, dist.Crime=ave(UP.Analysis$Crime, UP.Analysis$DIST_NAME))
UP.Analysis<-transform(UP.Analysis, dist.Incmbnt=ave(UP.Analysis$Incumbent.Party, UP.Analysis$DIST_NAME))
UP.Analysis<-transform(UP.Analysis, dist.gen=ave(UP.Analysis$GEN, UP.Analysis$DIST_NAME))
UP.Analysis<-transform(UP.Analysis, dist.electors=ave(UP.Analysis$Electors, UP.Analysis$DIST_NAME))
UP.Analysis<-transform(UP.Analysis, dist.polled=ave(UP.Analysis$Polled, UP.Analysis$DIST_NAME))
UP.Analysis$dist.turnout<-UP.Analysis$dist.polled/UP.Analysis$dist.electors
UP.Cluster<-UP.Analysis[,c(1,55:65)]
UP.Cluster<- UP.Cluster[!duplicated(UP.Cluster), ]
head(UP.Cluster)
randomorder<-runif(nrow(UP.Cluster))
UP.Cluster<-UP.Cluster[order(randomorder),]
UP.Cluster$Category<-ifelse(UP.Cluster$dist.gen==1,"GEN", "RES")
qplot(dist.electors,dist.TA, colour = Category,data=UP.Cluster)
Cluster.dist.2<-UP.Cluster[,c(1:10,12)]
variables<-colnames(Cluster.dist.2)[-1]
nmatrix<-scale(Cluster.dist.2[,variables])
ncenter<-attr(nmatrix, "scaled:center")
nscale<-attr(nmatrix, "scaled:scale")
z<-Cluster.dist.2[,-c(1,1)]
z
m<-apply(z,2,mean)
m
s<-apply(z,2,sd)
s
z1<-scale(z,m,s)
z1
distance<-dist(z1)
print(distance, digits=3)
hc.UP_c<-hclust(distance)
plot(hc.UP_c, labels=Cluster.dist.2$Dist.EEC)
print(hc.UP_c)
hc.UP_a<-hclust(distance, method="average")
plot(hc.UP_a, labels=Cluster.dist.2$Dist.EEC)
print(hc.UP_a)
members_hc.UP.c<-cutree(hc.UP_c,k=5)
members_hc.UP.a<-cutree(hc.UP_a,k=5)
table(members_hc.UP.c,members_hc.UP.a)
aggregate(z1,list(members_hc.UP.c),mean)
UP.work<-UP.Analysis[,c(2,18:23,31,36:45,48:54)]
set.seed(1234)
pd<-sample(2,nrow(UP.work),replace=TRUE, prob=c(0.7,0.3))
0.7*374
train<-UP.work[pd==1,]
val<-UP.work[pd==2,]
sum(UP.Analysis$Winner)
sum(val$Winner)
sum(train$Winner)
train.2fact<-train[,c(20,22,24)]
NB.1<-naiveBayes(x=train.2fact[-3], y=train.2fact$Status)
val.2fact<-val[,c(20,22,24)]
NB.1<-naiveBayes(x=train.2fact[-3], y=train.2fact$Status)
NB.1
setwd( "C:/Home/Work/GreatLakes/Machine Learning")
library(car)
library(caret)
library(class)
library(devtools)
library(e1071)
library(ggord)
library(ggplot2)
library(Hmisc)
library(klaR)
library(klaR)
library(MASS)
library(nnet)
library(plyr)
library(pROC)
library(psych)
library(scatterplot3d)
library(SDMTools)
library(dplyr)
library(ElemStatLearn)
library(rpart)
library(rpart.plot)
library(randomForest)
library(neuralnet)
normalize<-function(x){
+return((x-min(x))/(max(x)-min(x)))}
data(iris)
iris
qplot(Sepal.Length, Petal.Length,color=Species, data=iris)
iris.data<-iris[,c(1,3,5)]
iris.data$Sepal.Length<-normalize(iris.data$Sepal.Length)
iris.data$Petal.Length<-normalize(iris.data$Petal.Length)
iris.data$Spec<-ifelse(iris.data$Species=="virginica","virginica","others")
iris.data$Spec.1<-ifelse(iris.data$Species=="virginica",1,0)
str(iris.data)
iris.data$Spec<-as.factor(iris.data$Spec)
set.seed(1234)
pd<-sample(2,nrow(iris.data),replace=TRUE, prob=c(0.7,0.3))
train.iris<-iris.data[pd==1,]
val.iris<-iris.data[pd==2,]
train.iris.NB<-train.iris[,c(1,2,4)]
val.iris.NB<-val.iris[,c(1,2,4)]
NB.iris<-naiveBayes(x=train.iris.NB[-3], y=train.iris$Spec)
y_pred.NB<-predict(NB.iris,newdata=val.iris.NB[-3])
y_pred.NB
cm.iris.NB=table(val.iris.NB[,3],y_pred.NB)
cm.iris.NB
library(ElemStatLearn)
set = val.iris.NB
X1 = seq(min(set[, 1])-1 , max(set[, 1]) +1, by = 0.01)
X2 = seq(min(set[, 2])-1 , max(set[, 2])+1 , by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Sepal.Length', 'Petal.Length')
y_grid = predict(NB.iris, newdata = grid_set)
plot(set[, -3],
main = 'Naive Bayes: Test.Iris ',
xlab = 'Sepal.Length', ylab = 'Petal.Length',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == "virginica", 'blue', 'red'))
points(set, pch = 21, bg = ifelse(set[, 3] == "virginica", 'black', 'green'))
train.iris.knn<-train.iris[,c(1,2,4)]
val.iris.knn<-val.iris[,c(1,2,4)]
y_pred.KNN<-knn(train.iris.knn[,-3],val.iris.knn[-3], cl=train.iris.knn[,3],k=3)
cm.knn<-table(val.iris.knn[,3],y_pred.KNN)
cm.knn
set = val.iris.knn
X1 = seq(min(set[, 1])-1 , max(set[, 1]) +1, by = 0.01)
X2 = seq(min(set[, 2])-1 , max(set[, 2])+1 , by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Sepal.Length', 'Petal.Length')
y_grid = knn(train=train.iris.knn[,-3],test=val.iris.knn[-3], cl=train.iris.knn[,3],k=3)
plot(set[, -3],
main = 'KNN: Test.Iris ',
xlab = 'Sepal.Length', ylab = 'Petal.Length',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == "virginica", 'blue', 'red'))
points(set, pch = 21, bg = ifelse(set[, 3] == "virginica", 'black', 'green'))
train.2fact<-train[,c(20,22,24)]
val.2fact<-val[,c(20,22,24)]
NB.1<-naiveBayes(x=train.2fact[-3], y=train.2fact$Status)
NB.1
y_pred<-predict(NB.1,newdata=val.2fact[-3])
cm.NB.1=table(val.2fact[,3],y_pred)
cm.NB.1
confusionMatrix(table(val.2fact[,3],y_pred))
library(ElemStatLearn)
set = train.2fact
X1 = seq(min(set[, 1])-0.1 , max(set[, 1])+0.1 , by = 0.005)
X2 = seq(min(set[, 2]) -0.1, max(set[, 2])+0.1 , by = 0.005)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('mean.assets', 'mean.studied')
y_grid = predict(NB.1, newdata = grid_set)
plot(set[, -3],
main = 'Naive Bayes (Training set)',
xlab = 'mean.assets', ylab = 'mean.studied',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == "Win", 'blue', 'red'))
points(set, pch = 21, bg = ifelse(set[, 3] == "Win", 'green', 'black'))
set = val.2fact
y_pred<-knn(train=train.2fact[,-3],test=val.2fact[-3], cl=train.2fact[,3],k=3)
cm.knn<-table(val.2fact[,3],y_pred)
cm.knn
confusionMatrix(table(val.2fact[,3],y_pred))
set = train.2fact
X1 = seq(min(set[, 1]) , max(set[, 1]) , by = 0.001)
X2 = seq(min(set[, 2]), max(set[, 2]) , by = 0.001)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('mean.assets',  'mean.studied')
y_grid = knn(train=train.2fact[,-3],test=grid_set[-3], cl=train.2fact[,3],k=3)
plot(set[, -3],
main = 'Knn (Training set)',
xlab = 'mean.assets', ylab = 'mean.studied',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == "Win", 'blue', 'red'))
points(set, pch = 21, bg = ifelse(set[, 3] == "Win", 'black', 'green'))
train.NB<-train[,c(4,6,7,12,13,20:22,24)]
val.NB<-val[,c(4,6,7,12,13,20:22,24)]
NB<-naiveBayes(x=train.NB[-9], y=train.NB$Status)
y_pred.NB<-predict(NB,newdata=val.NB[-9])
y_pred.NB
cm.NB=table(val.NB[,9],y_pred.NB)
cm.NB
confusionMatrix(table(val.NB[,9],y_pred.NB))
y_pred.3<-knn(train=train.NB[,-9],test=val.NB[-9], cl=train.NB[,9],k=3)
cm.knn.3<-table(val.NB[,9],y_pred.3)
cm.knn.3
confusionMatrix(table(val.NB[,9],y_pred.3))
y_pred.5<-knn(train=train.NB[,-9],test=val.NB[-9], cl=train.NB[,9],k=5)
cm.knn.5<-table(val.NB[,9],y_pred.5)
cm.knn.5
y_pred.7<-knn(train=train.NB[,-9],test=val.NB[-9], cl=train.NB[,9],k=7)
cm.knn.7<-table(val.NB[,9],y_pred.7)
cm.knn.7
y_pred.9<-knn(train=train.NB[,-9],test=val.NB[-9], cl=train.NB[,9],k=9)
cm.knn.9<-table(val.NB[,9],y_pred.9)
cm.knn.9
train.reg<-train[,c(1,4,6:9,11:13,15,19:22)]
val.reg<-val[,c(1,4,6:9,11:13,15,19:22)]
str(train.reg)
Linear<-Vote.Share~Incumbent.Party +Crime++Crorepati+ Major+ Grad+ Serious+Female+ Asset.Rank+mean.assets+ mean.age++mean.studied
Linear
LPM<-Winner~Incumbent.Party +Crime++Crorepati+ Major+ Grad+ Serious+Female+ Asset.Rank+mean.assets+ mean.age++mean.studied
Logit<-Winner~Incumbent.Party +Crime++Crorepati+ Major+ Grad+ Serious+Female+ Asset.Rank+mean.assets+ mean.age++mean.studied
OLS<-lm(Linear, data=train.reg)
summary (OLS)
vif(OLS)
vif(OLS) #
Linear.1<-Vote.Share~Incumbent.Party +Crorepati+  Grad+ Serious+Female+ Asset.Rank+mean.assets+ mean.age++mean.studied
OLS.1<-lm(Linear.1, data=train.reg)
summary (OLS.1)
vif(OLS.1)
val$Pred_OLS.1 <- predict(OLS.1, val.reg)
Pred.win<-ddply(val, .(AC_NAME), transform,
max.share=max(Pred_OLS.1))
Pred.win$win<-ifelse(Pred.win$Pred_OLS.1==Pred.win$max.share,1,0)
tabrg<-table(Pred.win$Winner, Pred.win$win > 0.5)
tabrg
sum(diag(tabrg))/sum(tabrg)
LPM<-lm(LPM,train.reg)
summary(LPM)
vif(LPM)
LPM.1<-Winner~Incumbent.Party +Crorepati+ Major+ Grad+ Serious+Female+ Asset.Rank+mean.assets+ mean.age++mean.studied
OLS.LPM1<-lm(LPM.1,train.reg)
summary(OLS.LPM1)
vif(OLS.LPM1)
LPM.2<-Winner~Incumbent.Party + Major+Female+  mean.age++mean.studied
OLS.LPM.2<-lm(LPM.2,train.reg)
summary(OLS.LPM.2)
Pred_LPM <- predict(OLS.LPM.2,val.reg)
tabLPM<-table(val.reg$Winner, Pred_LPM > 0.5)
tabLPM
tabLPM<-table(val.reg$Winner, Pred_LPM > 0.35)
tabLPM
Logit.1 <- glm(Logit   , train.reg, family = binomial)
summary(Logit.1)
vif(Logit.1)
Logit.2<-Winner~Incumbent.Party + Major+ Female+ mean.age++mean.studied
Logit.2 <- glm(Logit.2  , train.reg, family = binomial)
summary(Logit.2)
vif(Logit.2)
pred <- predict.glm(Logit.2, newdata=val.reg, type="response")
tab.logit<-confusion.matrix(val.reg$Winner,pred,threshold = 0.5)
tab.logit
roc<-roc(val.reg$Winner,pred)
plot(roc)
auc(val.reg$Winner,pred)
tab.logit<-confusion.matrix(val.reg$Winner,pred,threshold = 0.35)
tab.logit
train.dt<-train[,c(1,4,6:9,11:13,15,19:22,24)]
val.dt<-val[,c(1,4,6:9,11:13,15,19:22,24)]
DT<-Status~Incumbent.Party +Crime++Crorepati+ Major+ Grad+ Serious+Female+ Asset.Rank+mean.assets+ mean.age++mean.studied
RFM<-Status~Incumbent.Party +Crime++Crorepati+ Major+ Grad+ Serious+Female+ Asset.Rank+mean.assets+ mean.age++mean.studied
DT<-rpart(DT, method="class",train.dt)
rpart.plot(DT)
rpart.plot(DT, type=3,extra=101,fallen.leaves = T)
summary(DT)
set.seed(123)
folds<-createFolds(train$Winner,k=10)
str(folds)
Linear.kval<-Vote.Share~Incumbent.Party +Crorepati+  Grad+ Serious+Female+ Asset.Rank+mean.assets+ mean.age++mean.studied
cv_OLS<-lapply(folds,function(x){
train.ols.kval<-train.reg[x,]
test.ols.kval<-val.reg[-x,]
OLS.kval<-lm(Linear.kval   , train.ols.kval)
test.ols.kval$Pred_OLS.kval <- predict(OLS.kval, test.ols.kval)
Pred.win.kval<-ddply(test.ols.kval, .(AC_NAME), transform,
max.share=max(Pred_OLS.kval))
Pred.win.kval$win<-ifelse(Pred.win.kval$Pred_OLS.kval==Pred.win.kval$max.share,1,0)
tab.OLS.kval<-table(Pred.win.kval$Winner, Pred.win.kval$win > 0.5)
sum(diag(tab.OLS.kval))/sum(tab.OLS.kval)
})
str(cv_OLS)
fit.OLS<-mean(unlist(cv_OLS))
fit.OLS
LPM.2<-Winner~Incumbent.Party + Major+Female+  mean.age++mean.studied
cv_LPM<-lapply(folds,function(x){
train.lpm.kval<-train.reg[x,]
test.lpm.kval<-val.reg[-x,]
LPM.kval<-lm(LPM.2, train.lpm.kval)
LPM.kval.pred<-predict(LPM.kval, test.lpm.kval)
tab.LPM.kval<-table(test.lpm.kval$Winner, LPM.kval.pred>0.5)
sum(diag(tab.LPM.kval))/sum(tab.LPM.kval)
})
str(cv_LPM)
fit.LPM<-mean(unlist(cv_LPM))
fit.LPM
Logit.2<-Winner~Incumbent.Party + Major+ Female+ mean.age++mean.studied
cv_logit<-lapply(folds,function(x){
train.Logit.kval<-train.reg[x,]
test.Logit.kval<-val.reg[-x,]
Logit.kval<-glm(Logit.2   , train.Logit.kval, family = binomial)
pred.Logit.kval <- predict.glm(Logit.kval, newdata=test.Logit.kval, type="response")
tab.logit.kval<-confusion.matrix(test.Logit.kval$Winner,pred.Logit.kval,threshold = 0.5)
sum(diag(tab.logit.kval))/sum(tab.logit.kval)
})
str(cv_logit)
fit.logit.kval<-mean(unlist(cv_logit))
fit.logit.kval
normalize<-function(x){
+return((x-min(x))/(max(x)-min(x)))}
library(class)
cv_KNN.5<-lapply(folds,function(x){
train.NB.kval<-train.NB[x,]
test.NB.kval<-val.NB[-x,]
train.knn.kval<-as.data.frame(lapply(train.NB.kval[,c(1:8)],normalize))
test.knn.kval<-as.data.frame(lapply(test.NB.kval[,c(1:8)],normalize))
train_target.kval<-train.NB.kval[,9]
test_target.kval<-test.NB.kval[,9]
knn5.kval<-knn(train=train.knn.kval,test=test.knn.kval,cl=train_target.kval,k=5)
tab.knn.5.kval<-table(test_target.kval,knn5.kval)
sum(diag(tab.knn.5.kval))/sum(tab.knn.5.kval)
})
str(cv_KNN.5)
fit.KNN<-mean(unlist(cv_KNN.5))
fit.KNN
cv_NB<-lapply(folds,function(x){
train.NB.kval<-train.NB[x,]
test.NB.kval<-val.NB[-x,]
NB.kval<-naiveBayes(x=train.NB.kval[-9], y=train.NB.kval$Status)
y_pred.NB.kval<-predict(NB,newdata=test.NB.kval[-9])
cm.NB.kval=table(test.NB.kval[,9],y_pred.NB.kval)
sum(diag(cm.NB.kval))/sum(cm.NB.kval)
})
str(cv_NB)
fit.NB<-mean(unlist(cv_NB))
fit.NB
LDA<-Status~Incumbent.Party +Crime++Crorepati+ Major+ Grad+ Serious+Female+ Asset.Rank+mean.assets+ mean.age++mean.studied
cv_LDA<-lapply(folds,function(x){
train.lda.kval<-train.lda[x,]
test.lda.kval<-val.lda[-x,]
lda.kval<-lda(LDA   , train.lda.kval)
lda.kval.pred<-predict(lda.kval, newdata=test.lda.kval)
ldapredclass.kval<-lda.kval.pred$class
tab.LDA.kval<-table(ldapredclass.kval,test.lda.kval$Winner)
sum(diag(tab.LDA.kval))/sum(tab.LDA.kval)
})
str(cv_LDA)
fit.LDA<-mean(unlist(cv_LDA))
fit.LDA
install.packages("rdataviewer")
install.packages(c("JGR","Deducer","DeducerExtras"))
getwd()
setwd("C:\Home\Work\GreatLakes\FAR")
setwd("C:/Home/Work/GreatLakes/FAR")
file = read.csv(file = "05.loan_default.csv", header = T)
set.seed(100)
install.packages("caTools")
library(caTools)
splits = sample.split(file$bad_flag, SplitRatio = 2/3)
splits
file[splits]
file$bad_flag[4]
Training = subset(file, splits == TRUE)
Training
summary(Training)
str(Training)
table(Training$bad_flag)/length(Training)
table(Training$bad_flag)
splits = sample.split(file$bad_flag, SplitRatio = 0.75)
Training = subset(file, splits == TRUE)
str(Training)
table(Training$bad_flag)
table(file$bad_flag)
table(Training$bad_flag)
length(file)
nrow(file)
table(Training$bad_flag)/nrow(Training)
table(file$bad_flag)/nrow(file)
model = glm(bad_flag ~., data = Training, family = binomial)
summary(model)
model = glm(bad_flag ~., data = Training, family = binomial(Logit))
model = glm(bad_flag ~., data = Training, family = binomial(link = "logit"))
summary(model)
TestData = subset(file, splits == FALSE)
PredictTest = predict(model, newdata = TestData, type = "response")
PredictTest
head(PredictTest,10)
predictClass = PredictTest > 0.5
confusion_matrix = table(TestData$bad_flag,predictClass)
confusion_matrix
install.packages("DMwR")
library(DMwR)
TrainingS = SMOTE(bad_flag~.,data = Training, perc.over = 200, perc.under = 600)
TrainingS = Training
TrainingS$bad_flag = as.factor(TrainingS$bad_flag)
str(TrainingS)
TrainingSMOTE = SMOTE(bad_flag~.,data = TrainingS, perc.over = 500)
TrainingSMOTE
table(Training$bad_flag)/nrow(Training)
table(TrainingSMOTE$bad_flag)/nrow(Training)
table(TrainingSMOTE$bad_flag)/nrow(TrainingSMOTE)
model = glm(bad_flag ~., data = TrainingSMOTE, family = binomial(link = "logit"))
summary(model)
TestData = subset(file, splits == FALSE)
PredictTest = predict(model, newdata = TestData, type = "response")
head(PredictTest,10)
predictClass = PredictTest > 0.5
confusion_matrix = table(TestData$bad_flag,predictClass)
confusion_matrix
TrainingSMOTE = SMOTE(bad_flag~.,data = TrainingS, perc.over = 800)
model = glm(bad_flag ~., data = TrainingSMOTE, family = binomial(link = "logit"))
summary(model)
TestData = subset(file, splits == FALSE)
PredictTest = predict(model, newdata = TestData, type = "response")
head(PredictTest,10)
predictClass = PredictTest > 0.5
confusion_matrix = table(TestData$bad_flag,predictClass)
confusion_matrix
TrainingSMOTE = SMOTE(bad_flag~.,data = TrainingS, perc.over = 500)
TrainingSMOTE
table(file$bad_flag)/nrow(file)
table(Training$bad_flag)/nrow(Training)
table(TrainingSMOTE$bad_flag)/nrow(TrainingSMOTE)
model = glm(bad_flag ~., data = TrainingSMOTE, family = binomial(link = "logit"))
summary(model)
TestData = subset(file, splits == FALSE)
PredictTest = predict(model, newdata = TestData, type = "response")
head(PredictTest,10)
predictClass = PredictTest > 0.5
confusion_matrix = table(TestData$bad_flag,predictClass)
confusion_matrix
